{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUQnwz82ho7MK3gKyc/ySx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ab-bijoy/Detecting_AI-generated-product-reviews/blob/main/Data%20augmentation/Data_Augmentation_with_MuRIL(Tamil).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "XYUPU5QWwIn5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KFvUePq-uhvm"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.12.0 transformers pandas torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1Too1o1eIAazaM-XQXYKI_9gR8dxcoiYu/view?usp=sharing"
      ],
      "metadata": {
        "id": "KHCiAsxcXc7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Too1o1eIAazaM-XQXYKI_9gR8dxcoiYu\n",
        "!unzip -q Train.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BkphdiBeXVE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURE  FILES AND COLUMN**"
      ],
      "metadata": {
        "id": "JUNsGseqwSEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CSV_PATH = '/content/Train/tam_training_data_hum_ai.csv'\n",
        "TEXT_COLUMN_NAME = 'DATA'\n",
        "OUTPUT_CSV_PATH = 'augmented_output.csv'"
      ],
      "metadata": {
        "id": "7PZ7wRTxup3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loads the MuRIL fill-mask pipeline.**"
      ],
      "metadata": {
        "id": "f9t0h6z5wcWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "\n",
        "    print(\"Loading MuRIL model\")\n",
        "    try:\n",
        "        # Use GPU if available for significantly faster processing\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        unmasker = pipeline('fill-mask', model='google/muril-base-cased', device=device)\n",
        "        print(\" Model loaded successfully.\")\n",
        "        if device == 0:\n",
        "            print(\" Running on GPU for faster performance.\")\n",
        "        else:\n",
        "            print(\"Running on CPU. For large datasets, a GPU is recommended.\")\n",
        "        return unmasker\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading model: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "xGcToKkNusnv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performs contextual word substitution on a single sentence.**"
      ],
      "metadata": {
        "id": "jOgWY15-wn9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contextual_augmentation(sentence: str, unmasker_pipeline) -> str:\n",
        "    if not isinstance(sentence, str) or not unmasker_pipeline:\n",
        "        return sentence # Return original if input is not a string or model failed\n",
        "\n",
        "    words = sentence.split()\n",
        "    if len(words) <= 2: # Avoid augmenting very short sentences\n",
        "        return sentence\n",
        "\n",
        "    # Randomly select an index to mask (avoiding the first and last words for better context)\n",
        "    mask_index = random.randint(1, len(words) - 2)\n",
        "    original_word = words[mask_index]\n",
        "\n",
        "    # Create the masked sentence using the model's specific mask token\n",
        "    words[mask_index] = unmasker_pipeline.tokenizer.mask_token\n",
        "    masked_sentence = \" \".join(words)\n",
        "\n",
        "    try:\n",
        "        predictions = unmasker_pipeline(masked_sentence, top_k=5)\n",
        "    except:\n",
        "        # If the model fails for any reason, return the original sentence\n",
        "        return sentence\n",
        "\n",
        "    # Find a suitable replacement (not the same as the original word)\n",
        "    for pred in predictions:\n",
        "        predicted_token = pred['token_str'].strip()\n",
        "        if predicted_token.lower() != original_word.lower():\n",
        "            words[mask_index] = predicted_token\n",
        "            return \" \".join(words)\n",
        "\n",
        "    return sentence # Return original if no suitable replacement was found"
      ],
      "metadata": {
        "id": "fJXoIUTsuuoA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Execution**"
      ],
      "metadata": {
        "id": "C1Fau7uKwtIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load the augmentation model\n",
        "    unmasker = load_model()\n",
        "\n",
        "    if unmasker:\n",
        "        try:\n",
        "            # 2. Read the input CSV file\n",
        "            print(f\"\\nReading data from '{INPUT_CSV_PATH}'...\")\n",
        "            # Use on_bad_lines='skip' to handle potential parsing errors\n",
        "            df = pd.read_csv(INPUT_CSV_PATH, on_bad_lines='skip')\n",
        "            print(\" Input file read successfully.\")\n",
        "\n",
        "            # --- Add this line to inspect columns after reading ---\n",
        "            print(\"Columns after reading CSV:\", df.columns)\n",
        "            # -----------------------------------------------------\n",
        "\n",
        "            if TEXT_COLUMN_NAME not in df.columns:\n",
        "                raise ValueError(f\"Column '{TEXT_COLUMN_NAME}' not found in the CSV.\")\n",
        "\n",
        "            # 3. Apply the augmentation function to the specified column\n",
        "            print(f\"Augmenting text in the '{TEXT_COLUMN_NAME}' column... This may take time.\")\n",
        "\n",
        "            # Use tqdm for a progress bar\n",
        "            tqdm.pandas(desc=\"Augmenting rows\")\n",
        "            df['augmented_text'] = df[TEXT_COLUMN_NAME].progress_apply(\n",
        "                lambda text: contextual_augmentation(text, unmasker)\n",
        "            )\n",
        "\n",
        "            # Merge the original 'DATA' column and 'augmented_text' column into a single 'DATA' column\n",
        "            print(\"\\nMerging original and augmented text...\")\n",
        "            df['DATA'] = df['DATA'].astype(str) + \" \" + df['augmented_text'].astype(str)\n",
        "            print(\" Text merged successfully.\")\n",
        "\n",
        "\n",
        "            # Drop the 'augmented_text' column as it's now merged\n",
        "            df = df.drop(columns=['augmented_text'])\n",
        "            print(\" Dropped 'augmented_text' column.\")\n",
        "\n",
        "            # 4. Save the new DataFrame to an output CSV\n",
        "            print(f\"\\nSaving augmented data to '{OUTPUT_CSV_PATH}'...\")\n",
        "            df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
        "            print(f\" Process complete! Augmented file saved as '{OUTPUT_CSV_PATH}'.\")\n",
        "\n",
        "            # Display a sample of the result\n",
        "            print(\"\\n--- Sample of Augmented Data ---\")\n",
        "            print(df[[TEXT_COLUMN_NAME]].head())\n",
        "            print(\"--------------------------------\\n\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\" ERROR: The file '{INPUT_CSV_PATH}' was not found.\")\n",
        "        except ValueError as ve:\n",
        "            print(f\" ERROR: {ve}\")\n",
        "        except Exception as e:\n",
        "            print(f\" An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WYtLvgexuyWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WgNM5-xGu29v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the original and augmented dataframes\n",
        "original_df = pd.read_csv('/content/Train/tam_training_data_hum_ai.csv', on_bad_lines='skip')\n",
        "augmented_df = pd.read_csv('augmented_output.csv', on_bad_lines='skip')\n",
        "\n",
        "max_original_id_index = original_df.index.max() if not original_df.empty else -1\n",
        "augmented_df['ID'] = 'TAM_HUAI_TR_' + (augmented_df.index + max_original_id_index + 1).astype(str)\n",
        "\n",
        "# Concatenate the original and augmented dataframes\n",
        "merged_final_df = pd.concat([original_df, augmented_df], ignore_index=True)\n",
        "\n",
        "# Display the head of the merged dataframe and its info\n",
        "print(\"Merged Final DataFrame Head:\")\n",
        "display(merged_final_df.head())\n"
      ],
      "metadata": {
        "id": "163mFIjAu5pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Merged Final DataFrame Info**"
      ],
      "metadata": {
        "id": "tknC5CgVLn4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final_df.info()"
      ],
      "metadata": {
        "id": "RyjlNan3LmYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the final merged dataframe**"
      ],
      "metadata": {
        "id": "UvA4SF3yedyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final_df.to_csv('final_merged_augmented_data(tamil).csv', index=False, encoding='utf-8')\n",
        "print(\"\\nFinal merged data saved to 'final_merged_augmented_data.csv'\")"
      ],
      "metadata": {
        "id": "-60ilPg0u-QA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}