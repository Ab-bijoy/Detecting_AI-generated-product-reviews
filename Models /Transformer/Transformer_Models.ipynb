{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3wCYrFejtTeF0TLyBslWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ab-bijoy/Detecting_AI-generated-product-reviews/blob/main/Models%20/Transformer/Transformer_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "kejFwLg45TLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n"
      ],
      "metadata": {
        "id": "4ud-wswh5Sn3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1vNvW3eC4mtT-Qbs6T0uTmITnRkgOmKHr/view?usp=sharing"
      ],
      "metadata": {
        "id": "yy2pTdr8DFm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1vNvW3eC4mtT-Qbs6T0uTmITnRkgOmKHr\n"
      ],
      "metadata": {
        "id": "_EWu07ElDEzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Datasets.zip"
      ],
      "metadata": {
        "id": "pTL7nCHuDYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1. Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "llsVurMx5dZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(train_path, test_path):\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"Make sure the dataset files are in the correct path.\")\n",
        "\n",
        "    test_df.rename(columns={'Data': 'DATA', 'Label': 'LABEL'}, inplace=True)\n",
        "    train_df = train_df[['DATA', 'LABEL']]\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    train_df['DATA'] = train_df['DATA'].apply(clean_text)\n",
        "    test_df['DATA'] = test_df['DATA'].apply(clean_text)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df['LABEL'] = label_encoder.fit_transform(train_df['LABEL'])\n",
        "    test_df['LABEL'] = label_encoder.transform(test_df['LABEL'])\n",
        "\n",
        "    return train_df, test_df, label_encoder"
      ],
      "metadata": {
        "id": "NNZBEME75aaT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset Class**"
      ],
      "metadata": {
        "id": "p8hinwq35m9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "5WRFzZN55r8f"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Metric Calculation**"
      ],
      "metadata": {
        "id": "j1RTj43w6oLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "2Q008pR-6sq_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Setup**"
      ],
      "metadata": {
        "id": "NV01QPz87KML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_and_tokenizer(model_name, num_labels):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "z0Y_Guo96wc-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Training Arguments**"
      ],
      "metadata": {
        "id": "taoIa1Nh7P8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_arguments():\n",
        "    return TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lw-LeJCX6zpO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Trainer Setup**"
      ],
      "metadata": {
        "id": "vI3A6gVH7VkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(model, tokenizer, train_df, test_df, label_encoder):\n",
        "    train_dataset = TextClassificationDataset(\n",
        "        texts=train_df['DATA'].tolist(),\n",
        "        labels=train_df['LABEL'].tolist(),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    test_dataset = TextClassificationDataset(\n",
        "        texts=test_df['DATA'].tolist(),\n",
        "        labels=test_df['LABEL'].tolist(),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    training_args = get_training_arguments()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "XrrAvpkY62Vo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Evaluation**"
      ],
      "metadata": {
        "id": "ZmYohqHt7eA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(trainer):\n",
        "    results = trainer.evaluate()\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "    print(f\"F1 Score : {results['eval_f1']:.4f}\")\n",
        "    print(f\"Precision: {results['eval_precision']:.4f}\")\n",
        "    print(f\"Recall   : {results['eval_recall']:.4f}\")\n",
        "    print(\"--------------------------\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "1xNqCBic65WO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline for indic-bert : https://huggingface.co/ai4bharat/indic-bert**"
      ],
      "metadata": {
        "id": "G2E5xMNm7_YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tamil**"
      ],
      "metadata": {
        "id": "aDkJMcA5E8qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(tamil).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/tamil-test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"ai4bharat/indic-bert\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bFQy2Y4s7BCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Malayalam**"
      ],
      "metadata": {
        "id": "e0N8msJCFAkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(Mal).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/mal_test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"ai4bharat/indic-bert\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "-kx6-BLkEaMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline for muril-base-cased : https://huggingface.co/google/muril-base-cased**"
      ],
      "metadata": {
        "id": "4wF4ZzL486fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tamil**"
      ],
      "metadata": {
        "id": "IDJV6BCcD6aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(tamil).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/tamil-test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"google/muril-base-cased\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "wOnn2JnM8tvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Malayalam**"
      ],
      "metadata": {
        "id": "vXk-Q3LhEFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(Mal).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/mal_test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"google/muril-base-cased\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "tFvCioYREFHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline for xlm-roberta : https://huggingface.co/FacebookAI/xlm-roberta-base**"
      ],
      "metadata": {
        "id": "ZZN0f7C19XYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tamil**"
      ],
      "metadata": {
        "id": "95sA84VOEh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(tamil).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/tamil-test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "8tWrQVoY9EbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Malayalam**"
      ],
      "metadata": {
        "id": "ygBuBcTxElQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    TRAIN_PATH = \"/content/Datasets/final_merged_augmented_data(Mal).csv\"\n",
        "    TEST_PATH = \"/content/Datasets/mal_test.xlsx - Sheet1.csv\"\n",
        "    MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
        "\n",
        "    print(\"ðŸ”¹ Loading and preprocessing data...\")\n",
        "    train_df, test_df, label_encoder = load_and_preprocess_data(TRAIN_PATH, TEST_PATH)\n",
        "\n",
        "    print(\"ðŸ”¹ Initializing tokenizer and model...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    print(\"ðŸ”¹ Starting training...\")\n",
        "    trainer = train_model(model, tokenizer, train_df, test_df, label_encoder)\n",
        "\n",
        "    print(\"ðŸ”¹ Evaluating model...\")\n",
        "    evaluate_model(trainer)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "GWivyD9SEkvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}